{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from pickle import dump, load\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "swiss-simon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, LambdaLR\n",
    "from sklearn import metrics\n",
    "from collections import deque\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "irish-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = Path(\"/home/trytolose/rinat/kaggle/grav_waves_detection/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forbidden-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000e74ad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f4945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000661522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00007a006a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000a38978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  target\n",
       "0  00000e74ad       1\n",
       "1  00001f4945       0\n",
       "2  0000661522       0\n",
       "3  00007a006a       0\n",
       "4  0000a38978       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_PATH / \"training_labels.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forward-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list((INPUT_PATH / \"train\").rglob(\"*.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "practical-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [15:20<00:00, 108.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(69)\n",
    "# sample = np.random.choice(files, 100000, replace=False)\n",
    "# sample_ar = []\n",
    "\n",
    "# for s in tqdm(sample):\n",
    "#     sample_ar.append(np.load(s).T)\n",
    "# sample_ar = np.concatenate(sample_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "personalized-sector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(feature_range=(-1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler.fit(sample_ar)\n",
    "# sample_ar = scaler.fit_transform(sample_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blind-riding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pending-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "grand-explosion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4096)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample = np.load(\"/hdd2/kaggle/g2net/input/train/0/0/0/00000e74ad.npy\")\n",
    "x_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weighted-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wave_Block_true(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_channels,out_channels,dilation_rates):\n",
    "        super(Wave_Block_true,self).__init__()\n",
    "        self.num_rates = dilation_rates\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        self.dil_convs = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(nn.Conv1d(in_channels,out_channels,kernel_size=1))\n",
    "        dilation_rates = [2**i for i in range(dilation_rates)]\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.dil_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate, dilation=dilation_rate))\n",
    "            self.filter_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=1))\n",
    "            self.gate_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=1))\n",
    "            self.convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=1))\n",
    "            \n",
    "        self.end_block = nn.Sequential(nn.ReLU(), nn.Conv1d(out_channels,out_channels,kernel_size=1), nn.ReLU(), nn.Conv1d(out_channels,out_channels,kernel_size=1))\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.convs[0](x)\n",
    "#         res = x\n",
    "        skip = 0\n",
    "        for i in range(self.num_rates):\n",
    "            \n",
    "            res = x\n",
    "            x = self.dil_convs[i](x)\n",
    "            x = torch.mul(torch.tanh(self.filter_convs[i](x)), torch.sigmoid(self.gate_convs[i](x))) \n",
    "            x = self.convs[i+1](x)\n",
    "            skip = skip + x\n",
    "            #x += res\n",
    "            x = x + res\n",
    "        \n",
    "        x = self.end_block(skip)\n",
    "        return x\n",
    "\n",
    "class Andrewnet_v3_true(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.first_conv = nn.Sequential(nn.Conv1d(in_channels,64,7,padding=3), nn.BatchNorm1d(64), nn.ReLU())\n",
    "        self.waveblock_1 = nn.Sequential(Wave_Block_true(64, 16, 12), nn.BatchNorm1d(16))\n",
    "        self.waveblock_2 = nn.Sequential(Wave_Block_true(16, 32, 8), nn.BatchNorm1d(32))\n",
    "        self.waveblock_3 = nn.Sequential(Wave_Block_true(32, 64, 4), nn.BatchNorm1d(64))\n",
    "        self.waveblock_4 = nn.Sequential(Wave_Block_true(64, 128, 1), nn.BatchNorm1d(128))\n",
    "        self.waveblock_5 = nn.Sequential( \n",
    "                                        nn.Conv1d(128,128,7,padding=3), \n",
    "                                        nn.BatchNorm1d(128), \n",
    "                                        nn.ReLU())\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.first_conv(x)\n",
    "        x = self.waveblock_1(x)\n",
    "        x = self.waveblock_2(x)\n",
    "        x = self.waveblock_3(x)\n",
    "        x = self.waveblock_4(x)\n",
    "        x = self.waveblock_5(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x.view(-1, 128))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fatty-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.tensor(x_sample).unsqueeze(0).float()\n",
    "# model = Andrewnet_v3_true(3)\n",
    "# pred = model(x)\n",
    "# print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-pendant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compressed-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_DICT = {x.stem: str(x) for x in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extra-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=69)\n",
    "df['fold'] = -1\n",
    "for f, (train_ids, val_ids) in enumerate(skf.split(df.index, y=df['target'])):\n",
    "    df.loc[val_ids, 'fold'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "committed-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"path\"] = df[\"id\"].apply(lambda x: FILE_PATH_DICT[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caroline-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, df, scaler):\n",
    "        self.df = df\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def __len__(self):     \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"path\"]\n",
    "        target = float(self.df.loc[idx, \"target\"])\n",
    "        x = np.load(path)\n",
    "        return x.max(axis=1), x.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "protected-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(INPUT_PATH / \"training_labels.csv\")\n",
    "# files = list((INPUT_PATH / \"train\").rglob(\"*.npy\"))\n",
    "# FILE_PATH_DICT = {x.stem: str(x) for x in files}\n",
    "# df[\"path\"] = df[\"id\"].apply(lambda x: FILE_PATH_DICT[x])\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH / \"sample_submission.csv\")\n",
    "files = list((INPUT_PATH / \"test\").rglob(\"*.npy\"))\n",
    "FILE_PATH_DICT = {x.stem: str(x) for x in files}\n",
    "df[\"path\"] = df[\"id\"].apply(lambda x: FILE_PATH_DICT[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "moved-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 3532/3532 [00:18<00:00, 192.53it/s]\n"
     ]
    }
   ],
   "source": [
    "BS = 64\n",
    "FOLD = 0\n",
    "\n",
    "scaler = load(open('scaler.pkl', 'rb'))\n",
    "train_ds = SignalDataset(df, scaler)\n",
    "# val_ds = SignalDataset(df[df['fold']==FOLD].reset_index(drop=True)[:1000], scaler)\n",
    "\n",
    "train_loader = utils.DataLoader(train_ds, shuffle=False, num_workers=11, batch_size=BS, pin_memory=True)\n",
    "# val_loader = utils.DataLoader(val_ds, shuffle=False, num_workers=11, batch_size=BS, pin_memory=True)\n",
    "\n",
    "\n",
    "# model = Andrewnet_v3_true(3)\n",
    "\n",
    "# model.cuda()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', verbose=True, patience=5, factor=0.5, eps=1e-12)\n",
    "\n",
    "train_max, train_min = [], []\n",
    "# best_score = 0\n",
    "# best_models = deque(maxlen=5)\n",
    "for e in range(1):\n",
    "\n",
    "    # Training:\n",
    "#     train_loss = []\n",
    "#     model.train()\n",
    "    \n",
    "    for x, y in tqdm(train_loader, ncols = 70):\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        train_max.append(x)\n",
    "        train_min.append(y)\n",
    "#         optimizer.zero_grad()\n",
    "#         x = x.cuda().float()\n",
    "#         y = y.cuda().float().unsqueeze(1)\n",
    "#         pred = model(x)\n",
    "#         loss = loss_fn(pred, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "#     val_loss = []\n",
    "\n",
    "#     val_true = []\n",
    "#     val_pred = []\n",
    "#     model.eval()  \n",
    "#     with torch.no_grad():\n",
    "#         for x, y in tqdm(val_loader, ncols=50):\n",
    "#             x = x.cuda().float()\n",
    "#             y = y.cuda().float().unsqueeze(1)\n",
    "#             pred = model(x)\n",
    "#             loss = loss_fn(pred, y)\n",
    "#             val_loss.append(loss.item())\n",
    "            \n",
    "#             pred = pred.sigmoid().cpu().data.numpy()\n",
    "#             val_pred.append(pred)\n",
    "#             val_true.append(y.cpu().numpy())\n",
    "    \n",
    "#     val_loss = np.mean(val_loss)\n",
    "    \n",
    "#     val_true = np.concatenate(val_true).reshape(-1,)\n",
    "#     val_pred = np.concatenate(val_pred).reshape(-1,)\n",
    "    \n",
    "#     final_score = metrics.roc_auc_score(val_true, val_pred)\n",
    "        \n",
    "    # print(f'Epoch: {e:03d}; lr: {lr:.06f}; train_loss: {np.mean(train_loss):.05f}; val_loss: {val_loss:.05f}; ', end='')\n",
    "#     print(f'Epoch: {e:03d}; train_loss: {np.mean(train_loss):.05f} val_loss: {val_loss:.05f}; roc: {final_score:.5f}', end=' ')\n",
    " \n",
    "#     if final_score > best_score:\n",
    "#         best_score = final_score\n",
    "#         torch.save(model.state_dict(), f\"baseline_f0.pt\")        \n",
    "#     else:\n",
    "#         print()\n",
    "\n",
    "    # print(metrics.classification_report(val_true, val_pred))\n",
    "        \n",
    "#     scheduler.step(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "foreign-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max = np.concatenate(train_max)\n",
    "train_min = np.concatenate(train_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "included-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_max = [4.61521162e-20, 4.14383536e-20, 1.11610637e-20]\n",
    "#train_min = [-4.42943562e-20, -4.23039083e-20, -1.08631992e-20]\n",
    "\n",
    "#test_max = [4.16750054e-20, 4.16596419e-20, 1.09645901e-20]\n",
    "#test_min = [-4.12508703e-20, -4.17404094e-20, -1.07887724e-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "australian-registrar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.12508703e-20, -4.17404094e-20, -1.07887724e-20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_min.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "computational-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "        \n",
    "    def __len__(self):     \n",
    "        return 250#len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rand_id = np.random.randint(len(self.df), size=1)[0]\n",
    "        return df.loc[rand_id, \"value\"]\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "df[\"value\"] = np.arange(500)\n",
    "\n",
    "    \n",
    "train_loader = utils.DataLoader(SignalDataset(df), shuffle=True, num_workers=10, batch_size=10, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "breeding-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "passing-synthetic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([272, 394, 436, 385, 134,  98, 118, 133, 371, 295]),\n",
       " tensor([ 17, 398, 205, 131, 145, 108, 417,  34, 409, 195]),\n",
       " tensor([328, 168, 225, 149,  42, 364, 492, 188, 426, 431]),\n",
       " tensor([147, 352,  79, 201,  93, 447, 138, 162,  30, 190]),\n",
       " tensor([376, 400,  97,  61, 325,  65, 438, 376, 105, 343]),\n",
       " tensor([ 15, 444, 324,  53, 445, 477, 329, 392, 432, 103]),\n",
       " tensor([427, 124, 318, 320, 319, 109, 341, 112, 110, 420]),\n",
       " tensor([ 89, 443, 142, 377,  65,  14,  58, 236,  78, 330]),\n",
       " tensor([ 84, 220, 353, 351, 171, 426, 492, 430, 271, 412]),\n",
       " tensor([193, 363, 208, 456, 163,  36, 212, 275, 474, 108]),\n",
       " tensor([162,  15,  35, 273,  48, 171,  10, 498, 235, 157]),\n",
       " tensor([346, 365, 484, 263, 281, 377, 172, 473, 161, 369]),\n",
       " tensor([135, 470,  78,  86, 326, 288, 312, 300, 245,  49]),\n",
       " tensor([  7, 111, 183, 274,  43, 200, 105, 295, 167, 343]),\n",
       " tensor([138,  26, 388, 186, 311, 183, 128, 303,  71, 200]),\n",
       " tensor([234,   0, 219, 474, 230, 328, 168, 220, 236, 340]),\n",
       " tensor([114, 221, 447, 116, 245, 249, 310, 165, 143, 193]),\n",
       " tensor([334,  28,   2,  70, 424, 236, 108, 127, 254, 380]),\n",
       " tensor([495, 334, 431, 177, 418, 267,  46, 150, 112, 198]),\n",
       " tensor([361, 303, 254, 397, 165, 460, 337, 319, 224, 314]),\n",
       " tensor([159,  82, 326, 320, 239, 241, 441, 317, 256, 350]),\n",
       " tensor([ 76, 236, 435, 402, 450,   1,  31, 281, 382,  62]),\n",
       " tensor([ 91, 238,  51,   6,  68,  51,  40,  49, 450, 328]),\n",
       " tensor([ 13, 232, 157, 495,  58, 219, 458, 333,  51, 331]),\n",
       " tensor([253, 296, 127, 297, 158, 423, 326, 322, 484, 100])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-disney",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-belgium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
